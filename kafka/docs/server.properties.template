# Apache Kafka 3.9.1 Server Configuration
# KRaft Mode - Production Settings for Financial Transactions
# 
# INSTRUCTIONS:
# 1. Copy this file to /opt/kafka/config/server.properties
# 2. Replace the placeholders:
#    - NODE_ID: Set to 1, 2, or 3 (unique for each server)
#    - NODE_HOSTNAME: Set to kafka1, kafka2, or kafka3
#    - CLUSTER_UUID: Use the UUID generated by kafka-storage.sh random-uuid
# 3. Save the file
# 4. Format storage: kafka-storage.sh format -t CLUSTER_UUID -c server.properties
# 5. Start Kafka: systemctl start kafka

########################### KRaft Mode Configuration ###########################

# REQUIRED: Set to broker,controller (combined mode)
# This means the server handles both data storage and cluster coordination
process.roles=broker,controller

# REQUIRED: Unique ID for this node (1, 2, or 3)
# Change this for each server: kafka1=1, kafka2=2, kafka3=3
node.id=NODE_ID

# REQUIRED: List of all controllers in the cluster
# Format: id@hostname:port,id@hostname:port,id@hostname:port
# This setting must be IDENTICAL on all three servers
controller.quorum.voters=1@kafka1:9094,2@kafka2:9094,3@kafka3:9094

########################### Network Configuration ###########################

# REQUIRED: Where this broker listens for connections
# PLAINTEXT: Regular unencrypted connections
# CONTROLLER: Internal communication between controllers
# Change NODE_HOSTNAME to match this server (kafka1, kafka2, or kafka3)
listeners=PLAINTEXT://NODE_HOSTNAME:9092,CONTROLLER://NODE_HOSTNAME:9094

# REQUIRED: Address advertised to clients
# This is what clients use to connect
# Change NODE_HOSTNAME to match this server (kafka1, kafka2, or kafka3)
advertised.listeners=PLAINTEXT://NODE_HOSTNAME:9092

# REQUIRED: Protocol used for broker-to-broker communication
inter.broker.listener.name=PLAINTEXT

# REQUIRED: Protocol used for controller communication
controller.listener.names=CONTROLLER

########################### Storage Configuration ###########################

# REQUIRED: Where Kafka stores topic data
# This is where your actual messages are stored
# Make sure this directory exists and is owned by kafka user
log.dirs=/data/kafka/logs

# REQUIRED: Where KRaft stores metadata
# This is cluster coordination data
# Make sure this directory exists and is owned by kafka user
metadata.log.dir=/data/kafka/metadata

########################### Cluster Identity ###########################

# REQUIRED: Unique identifier for this cluster
# Generate with: /opt/kafka/bin/kafka-storage.sh random-uuid
# This MUST be the same on all three servers
cluster.id=CLUSTER_UUID

########################### Replication Configuration ###########################

# CRITICAL: Number of replicas for new topics (default)
# Value: 3 = Every message is stored on all 3 servers
# Why: Ensures data safety - even if 2 servers fail, data is still available
default.replication.factor=3

# CRITICAL: Minimum replicas that must acknowledge a write
# Value: 2 = Leader + 1 replica must confirm before write is considered successful
# Why: Balance between data safety and availability
# - If set to 3: Can't write if any server is down
# - If set to 2: Can write even if 1 server is down
# - If set to 1: Risky - only leader confirms (data loss possible)
min.insync.replicas=2

########################### Consumer Offset Configuration ###########################

# CRITICAL: Replication factor for __consumer_offsets topic
# This was your specific requirement!
# Value: 3 = Consumer reading positions are stored on all 3 servers
# Why: If consumers crash, they can resume from last position even if servers fail
offsets.topic.replication.factor=3

# Number of partitions for __consumer_offsets topic
# Value: 50 = Good for hundreds of consumer groups
# More partitions = better scalability for many consumer groups
offsets.topic.num.partitions=50

# How long to keep consumer offset data (in minutes)
# Value: 10080 = 7 days
# Why: Keeps offset data even if consumers are offline for a week
offsets.retention.minutes=10080

########################### Transaction Configuration ###########################

# CRITICAL: Replication for transaction state log
# Value: 3 = Transaction states stored on all 3 servers
# Why: Ensures exactly-once delivery guarantees are maintained
transaction.state.log.replication.factor=3

# Minimum replicas for transaction state log
# Value: 2 = At least 2 replicas must be in sync
# Why: Balance between safety and availability
transaction.state.log.min.isr=2

########################### Topic Defaults ###########################

# Default number of partitions for new topics
# Value: 3 = Matches number of brokers
# Why: Allows even distribution across all 3 servers
# Note: Can be overridden when creating topics
num.partitions=3

# Whether topics are created automatically when messages are sent
# Value: false = Topics must be created explicitly
# Why: Prevents accidental topic creation in production
# Recommendation: Always create topics explicitly with proper settings
auto.create.topics.enable=false

########################### Network Threading ###########################

# Number of threads handling network requests
# Value: 8 = Good for most workloads
# Why: Handles multiple client connections simultaneously
# Formula: 1 thread per 1000 client connections (roughly)
num.network.threads=8

# Number of threads handling disk I/O operations
# Value: 16 = Good for most workloads
# Why: Handles reading/writing to disk in parallel
# Recommendation: Set to number of CPU cores or double num.network.threads
num.io.threads=16

########################### Socket Server Settings ###########################

# Size of send buffer (SO_SNDBUF) for socket server
# Value: 102400 bytes = 100 KB
# Why: Larger buffer = better throughput for large messages
socket.send.buffer.bytes=102400

# Size of receive buffer (SO_RCVBUF) for socket server
# Value: 102400 bytes = 100 KB
# Why: Larger buffer = better throughput
socket.receive.buffer.bytes=102400

# Maximum size of a request
# Value: 104857600 bytes = 100 MB
# Why: Allows large batch sizes from producers
socket.request.max.bytes=104857600

########################### Log Retention Policy ###########################

# How long to keep data before deletion
# Value: 168 hours = 7 days
# Why: Balances storage cost with data availability
# For financial transactions: Consider longer retention (e.g., 720 hours = 30 days)
log.retention.hours=168

# How often to check for old data to delete
# Value: 300000 ms = 5 minutes
# Why: Frequent enough to free space, not so frequent to waste resources
log.retention.check.interval.ms=300000

# Maximum size of a log segment file
# Value: 1073741824 bytes = 1 GB
# Why: Reasonable size for management and performance
# When a segment reaches this size, a new one is created
log.segment.bytes=1073741824

# How to handle old data
# Value: delete = Delete old data based on retention policy
# Alternative: compact = Keep only latest value for each key
log.cleanup.policy=delete

########################### Compression ###########################

# Default compression type for topics
# Value: lz4 = Fast compression with good ratio
# Why: LZ4 is fastest, uses least CPU
# Alternatives:
# - gzip: Higher compression, more CPU
# - snappy: Good balance
# - zstd: Best compression (Kafka 2.1+)
# - none: No compression
compression.type=lz4

########################### Group Coordinator Settings ###########################

# Timeout for consumer group members
# Value: 30000 ms = 30 seconds
# Why: If a consumer doesn't heartbeat for 30s, it's considered dead
group.min.session.timeout.ms=6000
group.max.session.timeout.ms=300000

########################### Internal Topic Settings ###########################

# Replication factor for internal topics
# These are created automatically by Kafka for cluster operation
offsets.topic.segment.bytes=104857600

########################### Log Flush Settings ###########################

# How many messages before forcing a flush to disk
# Value: not set (default behavior)
# Why: Let OS handle flushing for better performance
# Kafka relies on replication for durability, not disk flush
#log.flush.interval.messages=10000

# How often to flush to disk (time-based)
# Value: not set (default behavior)
# Why: Replication provides durability
#log.flush.interval.ms=1000

########################### Replica Management ###########################

# Time a follower can be behind before being removed from ISR
# Value: 10000 ms = 10 seconds
# Why: Prevents slow replicas from blocking writes
replica.lag.time.max.ms=10000

# Socket timeout for network requests
# Value: 30000 ms = 30 seconds
# Why: Allows time for large data transfers
replica.socket.timeout.ms=30000

# Buffer size for network requests
# Value: 65536 bytes = 64 KB
# Why: Reasonable size for replica fetches
replica.socket.receive.buffer.bytes=65536

# Maximum data to fetch in a single request
# Value: 1048576 bytes = 1 MB
# Why: Balances throughput and memory usage
replica.fetch.max.bytes=1048576

# How often to check if any replicas need to be shrunk
# Value: 10000 ms = 10 seconds
# Why: Keeps ISR updated reasonably quickly
replica.high.watermark.checkpoint.interval.ms=5000

########################### Controlled Shutdown ###########################

# Enable controlled shutdown
# Value: true = Gracefully transfer leadership before shutdown
# Why: Minimizes unavailability during restarts
controlled.shutdown.enable=true

# Maximum time to wait for controlled shutdown
# Value: 30000 ms = 30 seconds
# Why: Enough time for leadership transfer
controlled.shutdown.max.retries=3

########################### JMX Monitoring (Optional) ###########################

# Uncomment to enable JMX monitoring
# JMX allows external monitoring tools to collect metrics
#kafka.jmx.enable=true
#kafka.jmx.port=9999

########################### Additional Security Settings ###########################

# Uncomment and configure when enabling SSL/TLS
#listeners=PLAINTEXT://NODE_HOSTNAME:9092,SSL://NODE_HOSTNAME:9093,CONTROLLER://NODE_HOSTNAME:9094
#advertised.listeners=PLAINTEXT://NODE_HOSTNAME:9092,SSL://NODE_HOSTNAME:9093
#ssl.keystore.location=/opt/kafka/ssl/kafka.server.keystore.jks
#ssl.keystore.password=changeit
#ssl.key.password=changeit
#ssl.truststore.location=/opt/kafka/ssl/kafka.server.truststore.jks
#ssl.truststore.password=changeit
#ssl.client.auth=required

# Uncomment and configure when enabling SASL authentication
#sasl.enabled.mechanisms=SCRAM-SHA-512
#sasl.mechanism.inter.broker.protocol=SCRAM-SHA-512

########################### Notes ###########################

# BEFORE FIRST START:
# 1. Ensure all placeholders are replaced (NODE_ID, NODE_HOSTNAME, CLUSTER_UUID)
# 2. Create directories: /data/kafka/logs and /data/kafka/metadata
# 3. Set ownership: chown -R kafka:kafka /data/kafka
# 4. Format storage: kafka-storage.sh format -t CLUSTER_UUID -c this_file
# 5. Start service: systemctl start kafka

# VALIDATION:
# 1. Check logs: tail -f /var/log/kafka/server.log
# 2. Verify cluster: kafka-metadata-quorum.sh --bootstrap-server kafka1:9092 describe --status
# 3. List brokers: kafka-broker-api-versions.sh --bootstrap-server kafka1:9092

# FOR CHANGES:
# - Some settings can be changed dynamically (without restart)
# - Most settings require restart
# - Always test in development first!

########################### End of Configuration ###########################
